{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNv2SPPwIi9b4+4xlV6hpoO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilesh3030/Natural-Language-Processing/blob/main/Audio_Classification_multimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t2USJCKCIz_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Zf8_G-oDKxLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "excel_file_path = '/content/gdrive/MyDrive/7_KESDy18/SER-DB-ETRIv18_emotion_label_annotation.xlsx'\n",
        "\n",
        "# Read Excel file into a pandas DataFrame, without using any header\n",
        "df = pd.read_excel(excel_file_path, header=None)\n",
        "\n",
        "# Concatenate the first and second rows to create a single header row\n",
        "header_row = df.iloc[0] + '_' + df.iloc[1].astype(str)\n",
        "df.columns = header_row\n",
        "\n",
        "# Drop the first two rows as they are now included in the header\n",
        "df = df.drop([0, 1])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "8NKJMgaxKN9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "iIOvm4zKKOEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Total Evaluation_Emotion'].value_counts()"
      ],
      "metadata": {
        "id": "qS_dPVDEKOG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are few lables which have multiple emotions so for the simplicity of our analysis we will keep only those records which have a single emotion. Also we can see that data is skewed where neural emotion has almost 50% of the records so this is a clear case of imbalanced dataset."
      ],
      "metadata": {
        "id": "esricFOoqpYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Replace 'your_audio.wav' with the actual path to your .wav file\n",
        "audio_file_path = '/content/gdrive/MyDrive/7_KESDy18/004/dy/angry/004_dy_a_02.wav'\n",
        "\n",
        "# Load the audio file\n",
        "y, sr = librosa.load(audio_file_path, sr=None)\n",
        "\n",
        "# Calculate the time values for x-axis\n",
        "time = np.arange(0, len(y)) / sr\n",
        "\n",
        "# Plot the waveform\n",
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveshow(y, sr=sr)\n",
        "plt.title('Waveform of Audio')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v0acmYtdKOJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample rate of the audio file\n",
        "sr"
      ],
      "metadata": {
        "id": "CeBfPsOmLZFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MFCCs capture the essential spectral characteristics of the audio signal. The first few coefficients often represent the overall spectral shape, while higher-order coefficients capture finer details."
      ],
      "metadata": {
        "id": "KepKLNXsrbkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "print(mfccs.shape)"
      ],
      "metadata": {
        "id": "zEsUt3voLZKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function of extarct the MFCC features\n",
        "def extract_features(file_path):\n",
        "    audio, sample_rate = librosa.load(file_path)\n",
        "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
        "\n",
        "    return mfccs_scaled_features\n",
        "\n",
        "extract_features(audio_file_path)"
      ],
      "metadata": {
        "id": "aD39TJYpLZQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# root folder containing .wav files\n",
        "root_folder = '/content/gdrive/MyDrive/7_KESDy18/'\n",
        "\n",
        "# Create an empty list to store the results\n",
        "extracted_features=[]\n",
        "\n",
        "### Now we iterate through every audio file and extract features\n",
        "### using Mel-Frequency Cepstral Coefficients\n",
        "for root, dirs, files in os.walk(root_folder):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            file_path = os.path.join(root, file)\n",
        "            # Extract features and add them to the DataFrame\n",
        "            features = extract_features(file_path)\n",
        "            extracted_features.append([file,features])\n",
        "\n",
        "### converting extracted_features to Pandas dataframe\n",
        "extracted_features_df=pd.DataFrame(extracted_features,columns=['file','features'])\n",
        "extracted_features_df.head()"
      ],
      "metadata": {
        "id": "DSwSqpGbL30b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install googletrans==4.0.0-rc1\n",
        "# !pip install nltk"
      ],
      "metadata": {
        "id": "YX6ElKdcMwiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "UOmc1ZBdM5gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to translate text to English\n",
        "def translate_to_english(text):\n",
        "    translator = Translator()\n",
        "    translation = translator.translate(text, src='auto', dest='en')\n",
        "    return translation.text\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_tokens = [ps.stem(word) for word in filtered_tokens]\n",
        "\n",
        "    # Join tokens back to text\n",
        "    preprocessed_text = ' '.join(stemmed_tokens)\n",
        "\n",
        "    return preprocessed_text"
      ],
      "metadata": {
        "id": "coNFXYKOM5jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store file names and preprocessed text\n",
        "file_names = []\n",
        "preprocessed_texts = []\n",
        "\n",
        "# Loop through all text files in the folder\n",
        "for root, dirs, files in os.walk(root_folder):\n",
        "    for file_name in files:\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            # Construct the full path to the file\n",
        "            file_path = os.path.join(root, file_name)\n",
        "\n",
        "            # Read the text from the file using 'cp949' encoding\n",
        "            with open(file_path, 'r', encoding='cp949') as file:\n",
        "                text = file.read()\n",
        "\n",
        "            # Translate to English\n",
        "            translated_text = translate_to_english(text)\n",
        "\n",
        "            # Preprocess the text\n",
        "            preprocessed_text = preprocess_text(translated_text)\n",
        "\n",
        "            # Replace the extension with '.wav'\n",
        "            new_file_name = os.path.splitext(file_name)[0] + '.wav'\n",
        "\n",
        "            # Append file name and preprocessed text to lists\n",
        "            file_names.append(new_file_name)\n",
        "            preprocessed_texts.append(preprocessed_text)"
      ],
      "metadata": {
        "id": "dy5RwQBIM5mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store file names and preprocessed texts\n",
        "df_text = pd.DataFrame({'File_Name': file_names, 'Preprocessed_Text': preprocessed_texts})"
      ],
      "metadata": {
        "id": "TYvoknPkNafm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_text.shape)\n",
        "df_text.head()"
      ],
      "metadata": {
        "id": "nFoao6mmNf8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# column containing text\n",
        "text_data = df_text['Preprocessed_Text']\n",
        "\n",
        "# Calculate word lengths\n",
        "word_lengths = text_data.apply(lambda x: len(x.split()))\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(word_lengths, bins=20, color='blue', edgecolor='black')\n",
        "plt.xlabel('Word Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Word Lengths')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SzZbXAM8gl6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TeVuvk8urm0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see most of the text are less than 10 words so we will choose the max length of 10 while doing the padding for the inputs"
      ],
      "metadata": {
        "id": "KVWebis3rm7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner join on different columns\n",
        "merge_text_label = pd.merge(df_text, df, left_on='File_Name', right_on='Segment ID_nan', how='inner')\n",
        "final_df = pd.merge(merge_text_label, extracted_features_df, left_on='File_Name', right_on='file', how='inner')\n",
        "final_df.shape"
      ],
      "metadata": {
        "id": "wVwCYBTMPkph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head()"
      ],
      "metadata": {
        "id": "TNN1ol8Sg9LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter only relevant colmuns\n",
        "model_df = final_df[['features', 'Preprocessed_Text', 'Total Evaluation_Emotion']]\n",
        "\n",
        "# List of emotions to filter\n",
        "filter_values = ['neutral', 'sad', 'angry', 'happy', 'surprise', 'disgust', 'fear']\n",
        "\n",
        "# Filter the DataFrame based on the list of emotions\n",
        "filter_df = model_df[model_df['Total Evaluation_Emotion'].isin(filter_values)]"
      ],
      "metadata": {
        "id": "tdKFhmxMPksh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_df.shape"
      ],
      "metadata": {
        "id": "hjTzYyq5hRuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "max_text_length = 10\n",
        "\n",
        "def preprocess_data():\n",
        "    # pad text data\n",
        "    X_text = filter_df.Preprocessed_Text\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(X_text)\n",
        "\n",
        "    # Convert text data to integer sequences\n",
        "    X_text = tokenizer.texts_to_sequences(X_text)\n",
        "    X_text = np.array(pad_sequences(X_text, maxlen=max_text_length, padding='post').tolist())\n",
        "\n",
        "    # MFCC features\n",
        "    mfcc_data = np.array(filter_df.features.tolist())\n",
        "\n",
        "    # labels\n",
        "    labels = np.array(filter_df['Total Evaluation_Emotion'].tolist())\n",
        "\n",
        "    return X_text, mfcc_data, labels"
      ],
      "metadata": {
        "id": "TKO6X3IsPkvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, concatenate, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# Assuming you have X_text and X_mfcc as your preprocessed text and MFCC features, respectively\n",
        "X_text, X_mfcc, y = preprocess_data()\n",
        "\n",
        "# Train-Test split\n",
        "X_text_train, X_text_test, X_mfcc_train, X_mfcc_test, y_train, y_test = train_test_split(\n",
        "    X_text, X_mfcc, y, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = to_categorical(label_encoder.fit_transform(y_train))\n",
        "y_test_encoded = to_categorical(label_encoder.transform(y_test))"
      ],
      "metadata": {
        "id": "5C8Wj95Tixm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I have a preprocessed text input and MFCC feature input\n",
        "text_input = Input(shape=(max_text_length,), name='text_input')\n",
        "mfcc_input = Input(shape=(40,), name='mfcc_input')\n",
        "\n",
        "# text processing layers\n",
        "text_embedding = Embedding(input_dim=2000, output_dim=40)(text_input)\n",
        "text_flatten = Flatten()(text_embedding)\n",
        "\n",
        "# MFCC processing layers\n",
        "mfcc_dense = Dense(100, activation='relu')(mfcc_input)\n",
        "\n",
        "# Concatenate the processed text and MFCC features\n",
        "combined = concatenate([text_flatten, mfcc_dense])\n",
        "\n",
        "# output layer\n",
        "num_labels = y_train_encoded.shape[1]\n",
        "output = Dense(num_labels, activation='softmax')(combined)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[text_input, mfcc_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "# Model Checkpoint\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "# Training\n",
        "start = datetime.now()\n",
        "model.fit(\n",
        "    {'text_input': X_text_train, 'mfcc_input': X_mfcc_train},\n",
        "    y_train_encoded,\n",
        "    batch_size=32,\n",
        "    epochs=200,\n",
        "    validation_data=({'text_input': X_text_test, 'mfcc_input': X_mfcc_test}, y_test_encoded),\n",
        "    callbacks=[checkpointer],\n",
        "    verbose=1\n",
        ")\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ],
      "metadata": {
        "id": "-EBXy0_7Pkyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "test_accuracy = model.evaluate([X_text_test, X_mfcc_test], y_test_encoded, verbose=0)\n",
        "print(test_accuracy[1])\n",
        "\n",
        "# Prediction and Classification Report\n",
        "y_pred_probabilities = model.predict([X_text_test, X_mfcc_test])\n",
        "y_pred_labels = np.argmax(y_pred_probabilities, axis=1)\n",
        "y_true_labels = np.argmax(y_test_encoded, axis=1)\n",
        "report = classification_report(y_true_labels, y_pred_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "sdrffG_VkjGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this result, we should focus on the F1 scores instead of the accuracy. We can see F1 score is reasonable for all the labels but it has higher value for the classes where we had more number of records."
      ],
      "metadata": {
        "id": "bWKCt9J7r0XS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another complex model architecture"
      ],
      "metadata": {
        "id": "G7YtQVC8nSpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I have a preprocessed text input and MFCC feature input\n",
        "text_input = Input(shape=(max_text_length,), name='text_input')\n",
        "mfcc_input = Input(shape=(40,), name='mfcc_input')\n",
        "\n",
        "# Text processing layers\n",
        "text_embedding = Embedding(input_dim=2000, output_dim=40)(text_input)\n",
        "text_flatten = Flatten()(text_embedding)\n",
        "text_dense_1 = Dense(100, activation='relu')(text_flatten)\n",
        "text_dense_2 = Dense(50, activation='relu')(text_dense_1)\n",
        "text_dropout_2 = Dropout(0.5)(text_dense_2)\n",
        "\n",
        "# MFCC processing layers\n",
        "mfcc_dense_1 = Dense(100, activation='relu')(mfcc_input)\n",
        "mfcc_dense_2 = Dense(50, activation='relu')(mfcc_dense_1)\n",
        "mfcc_dropout_2 = Dropout(0.5)(mfcc_dense_2)\n",
        "\n",
        "# Concatenate the processed text and MFCC features\n",
        "combined = concatenate([text_dropout_2, mfcc_dropout_2])\n",
        "\n",
        "# Additional dense layer with dropout\n",
        "combined_dense = Dense(200, activation='relu')(combined)\n",
        "combined_dropout = Dropout(0.2)(combined_dense)\n",
        "\n",
        "# Output layer\n",
        "num_labels = y_train_encoded.shape[1]\n",
        "output = Dense(num_labels, activation='softmax')(combined_dropout)\n",
        "\n",
        "# Create the model\n",
        "model_complex = Model(inputs=[text_input, mfcc_input], outputs=output)\n",
        "\n",
        "\n",
        "# Define custom learning rate\n",
        "custom_learning_rate = 0.0001  # You can change this value\n",
        "\n",
        "# Create an instance of the Adam optimizer with the custom learning rate\n",
        "custom_optimizer = Adam(learning_rate=custom_learning_rate)\n",
        "\n",
        "# Compile the model\n",
        "model_complex.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=custom_optimizer)\n",
        "\n",
        "# Model Checkpoint\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/audio_classification_complex.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "# Training\n",
        "start = datetime.now()\n",
        "model_complex.fit(\n",
        "    {'text_input': X_text_train, 'mfcc_input': X_mfcc_train},\n",
        "    y_train_encoded,\n",
        "    batch_size=32,\n",
        "    epochs=500,\n",
        "    validation_data=({'text_input': X_text_test, 'mfcc_input': X_mfcc_test}, y_test_encoded),\n",
        "    callbacks=[checkpointer],\n",
        "    verbose=1\n",
        ")\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ],
      "metadata": {
        "id": "7BJV7r--nWfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "test_accuracy = model_complex.evaluate([X_text_test, X_mfcc_test], y_test_encoded, verbose=0)\n",
        "print(test_accuracy[1])\n",
        "\n",
        "# Prediction and Classification Report\n",
        "y_pred_probabilities = model_complex.predict([X_text_test, X_mfcc_test])\n",
        "y_pred_labels = np.argmax(y_pred_probabilities, axis=1)\n",
        "y_true_labels = np.argmax(y_test_encoded, axis=1)\n",
        "report = classification_report(y_true_labels, y_pred_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "GmFDzmtyne4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making a complex model didn't help much as we are short on the tariing dataset so to make a better model we need more records and make the dataset balanced."
      ],
      "metadata": {
        "id": "H9BOiBzlsM_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Comments:\n",
        "- Our model has shown a reasobale perfromanec on a very small set of data. Considering this data was highly imbalanced made it challenging to get a high accuracy or F1 score.\n",
        "- Additionally we can use the SMOTE techniques to make the dataset balanced and run models on the balanced dataset.\n",
        "- Since we were dealing with the translation of the korean texts so it would be better if we can use the english texts as the model inputs.\n",
        "- Regarding the deployment of model, the trained model is not very big so it can be deployed easily."
      ],
      "metadata": {
        "id": "vwehtdf0sYC7"
      }
    }
  ]
}